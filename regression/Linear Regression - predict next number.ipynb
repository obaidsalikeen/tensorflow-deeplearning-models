{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using TensorFlow.\n",
    "1. We will use raw TensorFlow to fit a line to our dataset. We will use random numbers as a dataset (contains 1 feature).\n",
    "2. Extend the example to perform linear regression on 1million examples with the help of batches (provide batches during train)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits:\n",
    "These examples are inspired by following online Udemy course: \n",
    "- Complete Guide to TensorFlow for Deep Learning with Python by Jose Portilla\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple linear regression using a small dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.03279726, 1.61501938, 1.00431687, 1.76409594, 3.26529081,\n",
       "       4.03621478, 3.4560377 , 5.10301563, 5.23256141, 4.6430358 ,\n",
       "       5.84529729, 5.91479022, 6.04799533, 7.72909826, 8.56254539,\n",
       "       7.56997048, 7.74607024, 9.25972842, 9.9843013 , 9.55247766])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.linspace(1, 10, 20) + np.random.uniform(high=1, low=-1, size=20)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.74122532,  1.54515411,  0.14130657,  0.97252958,  1.64260655,\n",
       "        5.34569266,  3.92476685,  4.63094762,  5.72874992,  5.43100498,\n",
       "        7.38945634,  7.44220692,  6.29620185,  6.58679211,  9.44308581,\n",
       "        7.47978947, 10.03934663, 10.37374243,  9.67896159, 11.68987749])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label = np.linspace(1,10,20) + np.random.uniform(high=2, low=-2, size=20)\n",
    "\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17a3ed464c8>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_data, y_label,'*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the cost function\n",
    "- y = wx + b\n",
    "\n",
    "We already have `x_data` and `y_label` as our training data. Both these are Numpy arrays, however `w` and `b` are Tensorflow variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(initial_value=0.85)\n",
    "w = tf.Variable(initial_value=0.60)\n",
    "\n",
    "# Just give any value to error. error is our cost function\n",
    "error = 0\n",
    "\n",
    "for x,y in zip(x_data, y_label):\n",
    "    new_y = w*x + b\n",
    "    error += (y-new_y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the optimizer for GradientDecent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bc735714b39c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcost_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m       raise RuntimeError(\n\u001b[1;32m--> 481\u001b[1;33m           \u001b[1;34m\"`loss` passed to Optimizer.compute_gradients should \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m           \"be a function when eager execution is enabled.\")\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "cost_function = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = cost_function.minimize(loss=error )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network - train `w` and `b`\n",
    "- Optimize the cost function `error`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trained_w = 0\n",
    "trained_b = 0\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    for x in range(epoch):\n",
    "        session.run(train)\n",
    "    \n",
    "    trained_w,trained_b = session.run([w,b])\n",
    "    \n",
    "    print(\" Trained values : trained_w {} trained_b {}\".format(trained_w, trained_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the decision boundary learned by the model (Line which fits the data best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trained = np.linspace(1,10, 20)\n",
    "y_trained = x_trained * trained_w + trained_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_label,'*')\n",
    "plt.plot(x_trained, y_trained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example of using batches to train with large dataset (1 million points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000000\n",
    "x_large = np.linspace(start=1, stop=10, num=dataset_size) + np.random.uniform(high=2, low=-2, size=dataset_size)\n",
    "y_large = np.linspace(start=1, stop=10, num=dataset_size) + np.random.uniform(high=2, low=-2, size=dataset_size)\n",
    "\n",
    "# Plot a random sample\n",
    "samples = 100\n",
    "random_sample_indexes = np.random.randint(0, high=dataset_size, size=samples)\n",
    "plt.plot(x_large[random_sample_indexes], y_large[random_sample_indexes], \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Variables\n",
    "batch_size = 8\n",
    "\n",
    "w = tf.Variable(0.86)\n",
    "b = tf.Variable(0.21)\n",
    "xPH = tf.placeholder(shape=[batch_size], dtype=tf.float32)\n",
    "yPH = tf.placeholder(shape=[batch_size], dtype=tf.float32)\n",
    "\n",
    "# Function:\n",
    "y_prediction =  w * xPH + b\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_sum(tf.square(yPH - y_prediction))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "cost_optimizer = optimizer.minimize(cost)\n",
    "\n",
    "# Train the model\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    for x in range(1000):\n",
    "        random_indexes = np.random.randint(0, high=dataset_size, size=batch_size)\n",
    "        feed_dict_map = {\n",
    "            xPH : x_large[random_indexes],\n",
    "            yPH : y_large[random_indexes]\n",
    "        }\n",
    "        session.run(cost_optimizer, feed_dict=feed_dict_map)\n",
    "        \n",
    "    w_trained, b_trained = session.run([w,b])\n",
    "\n",
    "    print(w_trained, b_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot samples using the trained `w` and `b` parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample_indexes = np.random.randint(0, high=dataset_size, size=samples)\n",
    "\n",
    "y_predicton = w_trained * x_large + b_trained\n",
    "\n",
    "plt.plot(x_large[random_sample_indexes], y_predicton[random_sample_indexes], 'r')\n",
    "plt.plot(x_large[random_sample_indexes], y_large[random_sample_indexes], '*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
